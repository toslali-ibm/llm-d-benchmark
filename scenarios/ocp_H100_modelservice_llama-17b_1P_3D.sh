export LLMDBENCH_VLLM_COMMON_AFFINITY=nvidia.com/gpu.product:NVIDIA-H100-80GB-HBM3
export LLMDBENCH_DEPLOY_MODEL_LIST=meta-llama/Llama-4-Scout-17B-16E-Instruct
export LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM=8
export LLMDBENCH_VLLM_COMMON_CPU_NR=16
export LLMDBENCH_VLLM_COMMON_CPU_MEM=64Gi
export LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN=250000
export LLMDBENCH_VLLM_COMMON_BLOCK_SIZE=128
export LLMDBENCH_VLLM_COMMON_MAX_NUM_BATCHED_TOKENS=32768
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_REPLICAS=1
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS="[--tensor-parallel-size____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM____--disable-log-requests____--max-model-len____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN____--distributed-executor-backend____mp____--block-size____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE____--max-num-batched-tokens____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_NUM_BATCHED_TOKENS]"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_REPLICAS=3
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS="[--tensor-parallel-size____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_TENSOR_PARALLELISM____--disable-log-requests____--max-model-len____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_MAX_MODEL_LEN____--distributed-executor-backend____mp____--block-size____REPLACE_ENV_LLMDBENCH_VLLM_COMMON_BLOCK_SIZE]"
