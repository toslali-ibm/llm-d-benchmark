load:
  type: constant
  stages:
  - rate: 1
    duration: 120
  - rate: 2
    duration: 120
  - rate: 4
    duration: 120
  - rate: 8
    duration: 120
api:
  type: completion
  streaming: true
server:
  type: vllm
  model_name: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL
  base_url: REPLACE_ENV_LLMDBENCH_HARNESS_STACK_ENDPOINT_URL
  ignore_eos: true
tokenizer:
  pretrained_model_name_or_path: REPLACE_ENV_LLMDBENCH_DEPLOY_CURRENT_MODEL
data:
  type: random
  input_distribution:
    min: 10             # min length of the synthetic prompts
    max: 8192           # max length of the synthetic prompts
    mean: 4096          # mean length of the synthetic prompts
    std: 2048           # standard deviation of the length of the synthetic prompts
    total_count: 1000   # total number of prompts to generate to fit the above mentioned distribution constraints
  output_distribution:
    min: 10             # min length of the output to be generated
    max: 2048           # max length of the output to be generated
    mean: 1024          # mean length of the output to be generated
    std: 512            # standard deviation of the length of the output to be generated
    total_count: 1000   # total number of output lengths to generate to fit the above mentioned distribution constraints
report:
  request_lifecycle:
    summary: true
    per_stage: true
    per_request: true
storage:
  local_storage:
    path: /workspace