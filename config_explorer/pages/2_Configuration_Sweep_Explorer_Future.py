"""
BLIS visualization page
"""
import subprocess
from matplotlib import pyplot as plt
import numpy as np
import plotly.express as px
import streamlit as st
from streamlit.delta_generator import DeltaGenerator
import pandas as pd
import util
from src.config_explorer.capacity_planner import *
import os
import importlib.util
import os, runpy, importlib.util
from pathlib import Path
import db

SIMULATE_BUTTON_KEY = 'simulate_button_key'

# import BLIS related modules and their path
spec = importlib.util.find_spec("request_rate_sweep")
SIMULATION_BASE_DIR = Path(spec.origin).parent
CONSTANTS_PATH = SIMULATION_BASE_DIR / "experiment_constants_inference.py"
RESULTS_PATH = SIMULATION_BASE_DIR / "results/sweep_params/simulator_inference_results.csv"

# Set the env variable expected by the BLIS script
os.environ["SIMULATION_BASE_DIR"] = str(SIMULATION_BASE_DIR)

def get_model_details(model_name, block_size, gpu_name, gpu_util):

    try: 
        gpu_memory_gb = db.gpu_specs[gpu_name]["memory"]
        model_info = get_model_info_from_hf(model_name)
        model_config = get_model_config_from_hf(model_name)
        
        max_model_len = model_config.max_position_embeddings

        ## per token memory
        num_layers = model_config.num_hidden_layers
        precision_in_bytes = precision_to_byte(inference_dtype(model_config))
        head_dimension = getattr(model_config, "head_dim", model_config.hidden_size / model_config.num_attention_heads)
        kv_heads = model_config.num_key_value_heads
        per_token_memory = num_layers * 2 * head_dimension * kv_heads * precision_in_bytes

        ## total memory per block
        mem_per_block = per_token_memory * block_size

        alloc_kv_gb = allocatable_kv_cache_memory(model_info, model_config, gpu_memory_gb, gpu_util/100)
        total_kv_blocks = int((alloc_kv_gb * (1024**3)) // mem_per_block)

        return max_model_len, total_kv_blocks
    
    except Exception as e:
        print("Err duiring capacity planning", e)
        raise NotImplementedError

def load_constants_as_dict():
    """Dynamically import experiment_constants.py and return its uppercase variables as a dict."""
    spec = importlib.util.spec_from_file_location("experiment_constants", CONSTANTS_PATH)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)

    # Get all uppercase variables (i.e., constants)
    config_dict = {
        k: getattr(module, k)
        for k in dir(module)
        if k.isupper()
    }
    return config_dict

def write_constants_from_dict(config_dict):
    """Write the CONFIG dict back to experiment_constants.py."""
    with open(CONSTANTS_PATH, "w") as f:
        f.write("# Experiment Constants - Generated by Streamlit App\n\n")
        for key, value in config_dict.items():
            if isinstance(value, str):
                f.write(f'{key} = "{value}"\n')
            else:
                f.write(f"{key} = {repr(value)}\n")

# --- Helpers ---
def parse_numeric_list(text: str, cast=int, fallback: List[int] | None = None) -> List[int]:
    """
    Parse a string like "50, 60 70;90" into a list of numbers (ints by default).
    Returns fallback if no valid values are found.
    """
    import re
    if not isinstance(text, str):
        return fallback or []
    tokens = [t for t in re.split(r"[,\s;]+", text.strip()) if t]
    values = []
    for t in tokens:
        try:
            # Support int-only by default but allow float cast if needed
            if cast is int:
                # Allow e.g. "50.0" to become 50
                values.append(int(float(t)))
            else:
                values.append(cast(t))
        except Exception:
            pass
    if not values:
        return fallback or []
    return values

def first_or_fallback(values: List[int], fallback: None | int=None) -> List[int]:
    """
    Return a single-value list: the first value if present, else [fallback].
    """
    if values:
        return [values[0]]
    if fallback is None:
        return []
    return [fallback]

def filter_numbers(numbers, threshold):
    """
    Returns a list of numbers less than or equal to the given threshold.

    Parameters:
    - numbers (list of int): The list of integers to filter.
    - threshold (int): The threshold value.

    Returns:
    - list of int: Filtered list with values <= threshold.
    """
    return [num for num in numbers if num <= threshold]

def inputs(tab: DeltaGenerator):
    """
    Inputs to the BLIS simulator
    """

    tab.subheader("Sweep input selection")
    tab.caption("Select various sweep options such as model and workload characteristics.")

    # Model
    models = ["Qwen/Qwen2.5-7B", "Qwen/Qwen3-14B"]

    with tab.container(border=True):
        selected_model = st.selectbox("Select a model",
                    options=models
                    )


        if selected_model and selected_model != "":

            # TODO: RM this!!!
            hf_token = None

            if selected_model and selected_model != "":
                # Fetch model info
                try:
                    model_info = get_model_info_from_hf(selected_model)
                except Exception as e:
                    st.warning("Cannot access model information, see error below.")
                    st.warning(e)
                    return None

                # Fetch model config
                try:
                    model_config = get_model_config_from_hf(selected_model, hf_token=hf_token)
                except Exception as e:
                    e_str = str(e)
                    if "gated" in e_str:
                        st.warning("This is a gated model, please submit a HF token to view information")
                        hf_token = st.text_input("HF token")
                        if hf_token:
                            model_config = get_model_config_from_hf(selected_model, hf_token=hf_token)
                    else:
                        st.warning("Cannot access model config, see error below.")
                        st.warning(e)
                        return None

                try:
                    model_gpu_memory_req = round(model_memory_req(model_info), 2)
                except Exception as e:
                    st.warning(f"Cannot retrieve relevant information about the model, {e}. The Capacity Planner only has partial information and functionality.")
                    return None

    # Scenario
    with tab.container(border=True):
        st.write("**Workload Profiles**")
        st.caption("Define the type of workload for the LLM. Select from a set of pre-defined workloads or tune each parameter based on your need.")

        preset_scenarios = {
            "Summarization": {
                "dataset": "Random Summarization",
                "request_rate": 25,
                "input_len": 5000,
                "output_len": 1000,
                "prefix_hit_ratio": 30,
                "latency_p50": 12000,
                "latency_p99": 15000,
                "throughput": 0.98,
                "ttft": 2000,
                "itl": 50,
                },
            "Chatbot": {
                "dataset": "Chatbot Dataset",
                "request_rate": 5,
                "input_len": 2048,
                "output_len": 256,
                "prefix_hit_ratio": 30,
                "latency_p50": 7500,
                "latency_p99": 10000,
                "throughput": 2,
                "ttft": 2000,
                "itl": 500,
                },
            "Classification": {
                "dataset": "Text Dataset",
                "request_rate": 5,
                "input_len": 256,
                "output_len": 5,
                "prefix_hit_ratio": 30,
                "latency_p50": 50,
                "latency_p99": 68,
                "throughput": 5,
                "ttft": 20,
                "itl": 5,
                },
        }

        datasets = [
            "Random Summarization",
            "Chatbot Dataset",
            "Text Dataset",
            "Synthetic",
            "Dataset 5",
            "Dataset 6",
        ]


        selected_workload = st.radio("Select workload",
                     options=preset_scenarios.keys())

        info = preset_scenarios[selected_workload]
        dataset = info['dataset']
        request_rate = info['request_rate']
        isl = info['input_len']
        osl = info['output_len']
        prefix_hit_ratio = info['prefix_hit_ratio']
        latency_p50 = info['latency_p50']
        latency_p99 = info['latency_p99']
        throughput = info['throughput']
        # ttft = info['ttft']
        # itl = info['itl']

        input_mean = info["input_len"]
        output_mean = info["output_len"]
        prefix_hit_ratio = info["prefix_hit_ratio"]

        st.write(f"""
- Dataset: {dataset}
- Input length: {isl}
- Output length: {osl}
- Prefix Hit Ratio: {prefix_hit_ratio}%
""")

        concurrency_options = [1,5,10]# benchmark_data["Concurrency"].unique()
        concurrency_options.sort()
        concurrency_selected = st.multiselect("Select concurrency (request rate)",
                                              options=concurrency_options,
                                              default=concurrency_options,
                       )

    # Environment & Hardware Section
    with tab.container(border=True):
        st.write("**Environment & Hardware**")

        # GPU Configuration
        accelerators = ["NVIDIA-H100-80GB-HBM3"]
        gpu_type = st.selectbox("Accelerator Type", accelerators) # db.gpu_specs.keys()
        num_gpus = st.number_input(
            "Total number of GPUs (this will filter out parallelism combinations that are invalid)",
            value=1,
            min_value=1)

    # vLLM parameters
    with tab.container(border=True):
        st.write("**vLLM parameters**")
        st.caption("Select what vLLM engine arguments to filter.")


        # -----------------------------
        # GPU Memory Utilization (%)
        # -----------------------------
        sweep_gpu_mem = st.checkbox(
            "GPU Memory Utilization (%)",
            value=True,
            help="Comma/space/semicolon-separated integers (1–100)."
        )

        default = [90]
        gpu_mem_text = st.multiselect(
            "Values to sweep (comma/space separated)",
            options=default,
            default=default,
            key="gpu_mem_text",
            disabled=not sweep_gpu_mem,
            label_visibility="collapsed",
        )
        gpu_memory_utilization_all = parse_numeric_list(
            gpu_mem_text, cast=int, fallback=default,
        )
        # Constrain to 1..100
        gpu_memory_utilization_all = [v for v in gpu_memory_utilization_all if 1 <= v <= 100]
        if sweep_gpu_mem and not gpu_memory_utilization_all:
            st.warning("Please provide at least one integer between 1 and 100.")
        # If not sweeping, select a single value (first or fallback)
        gpu_memory_utilization = (
            gpu_memory_utilization_all if sweep_gpu_mem
            else default
        )

        # -----------------------------
        # Block Size
        # -----------------------------
        sweep_block = st.checkbox(
            "Block Size",
            value=True,
            help="Comma/space/semicolon-separated integers (e.g., 16, 32)."
        )
        default = [16]
        block_size_text = st.multiselect(
            "Values to sweep (comma/space separated)",
            options=default,
            default=default,
            key="block_size_text",
            disabled=not sweep_block,
            label_visibility="collapsed",
        )
        block_size_all = parse_numeric_list(
            block_size_text, cast=int, fallback=default,
        )
       
        if sweep_block and not block_size_all:
            st.warning("Please provide at least one block size.")
        # If not sweeping, select a single value (first or fallback)
        block_size = (
            block_size_all if sweep_block
            else default
        )

        # -----------------------------
        # Long Prefill Token Threshold
        # -----------------------------
         
        sweep_chunk_sizes = st.checkbox(
            "Prefill Chunk Sizes (MAX_NUM_BATCHED_TOKENS)",
            value=True,
            help="Comma/space/semicolon-separated integers."
        )
        default_max_num_batched_tokens_text = "2048, 4096"
       
        max_num_batched_tokens_text = st.text_input(
            "Values to sweep (comma/space separated)",
            value=default_max_num_batched_tokens_text,
            key="long_prefill_text",
            disabled=not sweep_chunk_sizes,
            label_visibility="collapsed",
            help="Example: 256,512,1024,2048"
        )
        max_num_batched_tokens_all = parse_numeric_list(
            max_num_batched_tokens_text, cast=int, fallback=[256, 512, 1024, 2048]
        )
        if sweep_chunk_sizes and not max_num_batched_tokens_all:
            st.warning("Please provide at least one integer for Long Prefill Token Threshold.")
        max_num_batched_tokens_threshold = (
            max_num_batched_tokens_all if sweep_chunk_sizes
            else [2048]
        )

        # Parallelism selection

        st.divider()
        st.write("**Parallelism**")

        # -----------------------------
        # Tensor Parallelism (tp)
        # -----------------------------

        sweep_tp = st.checkbox(
            "Tensor Parallelism",
            value=True,
            help="Comma/space/semicolon-separated integers ≥ 1. Example: 1, 2, 4. TP=1 will always be considered.",
        )

        default = find_possible_tp(model_config)
        default = filter_numbers(default, num_gpus)

        tp = st.multiselect(
            "TP Values to sweep (comma/space separated)",
            options=default,
            default=default,
            key="tp_text",
            disabled=not sweep_tp,
            label_visibility="collapsed",
            help="Example: 1,2,4"
        )

        if not sweep_tp:
            tp = [1]

        # -----------------------------
        # Pipeline Parallelism (pp)
        # -----------------------------

        sweep_pp = st.checkbox(
            "Pipeline Parallelism",
            value=True,
            help="Comma/space/semicolon-separated integers ≥ 1. Example: 1, 2"
        )
        default = [1]

        pp = st.multiselect(
            "DP Values to sweep (comma/separated)",
            options=default,
            default=default,
            key="pp_text",
            disabled=not sweep_pp,
            label_visibility="collapsed",
            help="Example: 1,2"
        )

        if not sweep_pp:
            pp = [1]

        # -----------------------------
        # Data Parallelism (dp)
        # -----------------------------
        sweep_dp = st.checkbox(
            "Data Parallelism (replicas of model)",
            value=True,
            help="Comma/space/semicolon-separated integers ≥ 1. Example: 1, 2"
        )
        default_dp_text = [1]
        dp_text = st.multiselect(
            "Values to sweep (comma/space separated)",
            options=default_dp_text,
            default=default_dp_text,
            key="dp_text",
            disabled=not sweep_dp,
            label_visibility="collapsed",
            help="Example: 1,2"
        )
        dp_all = [v for v in parse_numeric_list(dp_text, cast=int, fallback=[1]) if v >= 1]
        if sweep_dp and not dp_all:
            st.warning("Please provide at least one integer ≥ 1 for Pipeline Parallelism (pp).")
        dp = dp_all if sweep_dp else first_or_fallback(dp_all, default_dp_text)

    # SLOs
    with tab.container(border=True):
        st.write("**Goals / SLOs**")
        st.caption("Define the desire constraints to reach for your application.")

        if selected_workload:
            scenario = preset_scenarios[selected_workload]
            disabled = selected_workload != "Custom"

            latency_col1, latency_col2 = st.columns(2)
            latency_p50 = latency_col1.number_input("E2E Median latency (ms)",
                                          value=scenario['latency_p50'],
                                          min_value=0,
                                          )
            latency_p99 = latency_col2.number_input("E2E latency p99 (ms)",
                                value=scenario['latency_p99'],
                                min_value=0,
                                )

            # ttft_col, itl_col = st.columns(2)
            # ttft = ttft_col.number_input("TTFT (ms)",
            #             value=scenario['ttft'],
            #             min_value=0,
            #             )
            # itl = itl_col.number_input("ITL (ms)",
            #             value=scenario['itl'],
            #             min_value=0,
            #             )

            throughput = st.number_input("Throughput (req/s)",
                                         value=scenario['throughput'],
                                         min_value=0.1,
                                         )

    data_to_return = {
        "model": selected_model,
        "gpu_type": gpu_type,
        "num_gpus": num_gpus,
        "tp": tp,
        "dp": dp,
        "pp": pp,
        "isl": isl,
        "osl": osl,
        "prefix_hit_ratio": prefix_hit_ratio,
        "input_mean": input_mean,
        "output_mean": output_mean,
        'concurrency': concurrency_selected,
        "gpu_memory_utilization": gpu_memory_utilization,
        # "max_num_batched_tokens": max_num_batched_tokens,
        "block_size": block_size,
        "max_num_batched_tokens_threshold": max_num_batched_tokens_threshold,
        # "enable_prefix_caching": enable_prefix_caching,
        "latency_p50": latency_p50,
        "latency_p99": latency_p99,
        "throughput": throughput,
        # "ttft": ttft,
        # "itl": itl,
        "throughput": throughput,
        # If value to sweep
        # "sweep_gpu_memory_utilization": sweep_gpu_mem,
        # "sweep_block_size": sweep_block_size,
        # "sweep_max_num_batched_tokens": sweep_max_tokens,
        # "sweep_long_prefill_token_threshold": sweep_long_prefill,
        # "sweep_enable_prefix_caching": sweep_prefix_cache,
    }

    return data_to_return

def output(tab, user_input: dict):
    """
    Visualize output
    """
    tab.subheader("Sweep exploration")
    tab.caption("Visualize performance results that meet input selection.")

   # --- Simulation button ---
    if tab.button("🚀 Simulate w/ BLIS", type="primary", use_container_width=False):
            # Create a placeholder for dynamic status updates
        status_placeholder = tab.empty()
        status_placeholder.info("Running BLIS simulation... this will take only a few seconds ⏳")

        try:
            # map CONFIG
            model_name = user_input["model"]
            gpu_util = user_input["gpu_memory_utilization"][0]
            gpu_type = user_input["gpu_type"]
            block_size = user_input["block_size"][0]

            # Utilize capacity_planner functions for max_model_len and total_kv_blocks
            max_model_len, total_kv_blocks = get_model_details(model_name, block_size, gpu_type, gpu_util)

            CONFIG["MODEL"] = model_name
            CONFIG["PREFIX_HIT_RATIOS"] = [user_input["prefix_hit_ratio"]/100]
            CONFIG["DATAGEN_SPECS"]["INPUT_LEN_MEAN"] = user_input["input_mean"]
            CONFIG["DATAGEN_SPECS"]["OUTPUT_LEN_MEAN"] = user_input["output_mean"]
            CONFIG["REQUEST_RATES"] = user_input["concurrency"] # [x / 100 for x in user_input["concurrency"]]
            CONFIG["GPU_MEM_UTIL"] = gpu_util/100 #0.9
            CONFIG["BLOCK_SIZE"] =  block_size #16
            CONFIG["MAX_NUM_BATCHED_TOKENS"] = user_input["max_num_batched_tokens_threshold"] # [256, 2048]
            CONFIG["GPU_TYPE"] =  gpu_type # "NVIDIA-H100-80GB-HBM3"
            
            model_key = model_name.split("/")[-1].replace(".", "_")
            CONFIG["TOTAL_KV_BLOCKS"][model_key] = total_kv_blocks # 
            CONFIG["CONTEXT_LENGTH"] = max_model_len

            write_constants_from_dict(CONFIG)

            # Remove old simulator results if they exist 
            if os.path.exists(RESULTS_PATH):
                try:
                    os.remove(RESULTS_PATH)
                except Exception as e:
                    None

            print("Running simulator")
            runpy.run_module("request_rate_sweep", run_name="__main__")
            
            print("reading results")
            df = pd.read_csv(RESULTS_PATH)
            
            # Display the CSV file as a table
            tab.subheader("All Simulation Results")
            tab.dataframe(df)  

            print(df)

            # Add the 'Score' column based on the SLO conditions
            latency_p50 = user_input["latency_p50"]
            latency_p99 = user_input["latency_p99"]
            throughput = user_input["throughput"]
            df["Score"] = df.apply(
                lambda row: int(
                    (row['Median E2E(ms)'] <= latency_p50) and
                    (row['P99 E2E(ms)'] <= latency_p99) and
                    (row['Request throughput (req/s)'] >= throughput)
                ), axis=1
            )
            
            # Filter the rows where 'Score' is 1 (indicating SLOs are met)
            meets_slo_df = df[df["Score"] == 1]

            # Display the filtered table (rows that meet the SLOs)
            tab.subheader("Configurations that Meet SLOs")
            tab.dataframe(meets_slo_df)  # Show the filtered DataFrame

            # Define the dimensions for the Plotly parallel categories chart
            dimensions = ["request_rate", "mbnt"]
            
            # Plotly Parallel Categories Chart
            fig = px.parallel_categories(
                df,
                dimensions=dimensions,
                color="Score",   # Color by the 'Score' column (0 = red, 1 = green)
                color_continuous_scale=[(0, "red"), (1, "green")],
                labels={col: col for col in df.columns}  # Label the columns nicely
            )
            
            # Display the Plotly chart
            tab.subheader("Configuration Chart for SLOs")
            tab.plotly_chart(fig, use_container_width=True)
            
            status_placeholder.success("✅ Simulation completed successfully!")
        except subprocess.CalledProcessError as e:
            status_placeholder.error(f"Simulation failed with error:\n\n{e}")
        except FileNotFoundError as e:
            status_placeholder.error("Could not find file. Check SIMULATION_BASE_DIR.", e)

if __name__ == "__main__":
    # Set up streamlit config
    st.set_page_config(page_title="Configuration Explorer",
                       page_icon=None,
                       layout="wide",
                       initial_sidebar_state="expanded",
                       menu_items=None)
    st.title("Configuration Explorer")
    st.caption("This tool helps you find the most cost-effective, optimal configuration for serving models on llm-d based on hardware specification, workload characteristics, and SLO requirements.")

    util.init_session_state()

    # Display Sweep Explorer headings
    st.header("Configuration Sweep Explorer (Future)")
    st.caption("Explore, exmaine, and visualize existing benchmarking data for optimal `llm-d` configurations.")

    # SIMULATION_BASE_DIR=/Users/toslali/Desktop/work/ibm/projects/llm-inference/study/inference-llmd/inference-sim python /Users/toslali/Desktop/work/ibm/projects/llm-inference/study/inference-llmd/inference-sim/request_rate_sweep.py

    CONFIG = load_constants_as_dict()

    col1, col2 = st.columns([0.3, 0.7], gap="large")
    col1_container = col1.container(height=1000, border=False)
    col2_container = col2.container(height=1000, border=False)
    user_inputs = inputs(col1_container)
    output(col2_container, user_inputs)
